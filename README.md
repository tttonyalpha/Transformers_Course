# Transformers in Deep Learning Course

Lecture and practice materials for each week are in ./week* folders. You can complete all asignments locally or in google colab (see readme files in week*)


# Syllabus
- __week01__ Attention Intro
  - [x] Lecture: Deep learning recap, embeddings, rnn bottelneck, from reccurence to attention 
  - [x] Seminar: Torch basics recap, Implementing Bahdanau attention

- __week02__ Transformers basics
  - [x] Lecture: QKW attention, FFN, residual connection, layer norm, positional encoding, dropout, tokenization, sampling.
  - [x] Seminar: Transformer implementation from scratch
  - [ ] Homework 1 is out!

- __week03__ Transformers in NLP
  - [ ] Lecture: Transfer learning, BERT (BART, RoBerta and etc.), GPT, T5.
  - [ ] Seminar: HF Transformers

- __week04__ Transformers in CV
  - [ ] Lecture: DETR, ViT, Swin.
  - [ ] Seminar: ViT deep dive

- __week05__ Transformers Training
  - [ ] Lecture: Pretraining objectives, prefix tuning, PEFT, LoRA, DPO
  - [ ] Seminar: HF TRL
  - [ ] Homework 2 is out!

- __week06__ Multimodal Transformers
  - [ ] Lecture: MLLM, image and text fusion, VLM
  - [ ] Seminar: vllm intro

- __week07__ Efficient Transformers
  - [ ] Lecture: KV cache, Prunning, Quantization, Distillation, GPU parallelism.
  - [ ] Seminar: 

- __week08__ Memory in Transformers
  - [ ] Lecture: Memory classification, TransformerXL, RMT, MemoryTransformer, RAG, Titans
  - [ ] Seminar: TransformerXL implementation
  - [ ] Homework 3 is out!

- __week09__ Transformers in other domains
  - [ ] Lecture: Audio, Robotics, Table ML, Bioligy
  - [ ] Seminar: SmolVLA overview

- __week10__ Exam consultation

(to be updated)

# Course staff
Course materials by
- [Nikita Kachaev](https://tttonyalpha.github.io) - lectures, seminars
